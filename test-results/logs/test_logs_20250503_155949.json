{
  "logs": [
    {
      "timestamp": "2025-05-03T15:59:49.017764",
      "level": "INFO",
      "message": "Starting test session",
      "metadata": {
        "test_dir": "/Users/admin/Documents/PersonalProjects/bittensor/analytics_framework/test-results"
      }
    },
    {
      "timestamp": "2025-05-03T15:59:49.197647",
      "level": "INFO",
      "message": "Test session completed",
      "metadata": {}
    },
    {
      "timestamp": "2025-05-03T15:59:49.645195",
      "level": "INFO",
      "message": "Test session finished with exit status 1",
      "metadata": {
        "exitstatus": 1,
        "test_counts": {
          "total": 1,
          "passed": 0,
          "failed": 1,
          "skipped": 0,
          "errors": 0
        },
        "duration": 0.6719591617584229,
        "failed_tests": [
          {
            "name": "tests/unit/miner/test_train.py::TestModelTraining::test_train_model",
            "duration": 0.05512120800267439,
            "error_message": "self = <test_train.TestModelTraining object at 0x1375f7f10>\nmock_adam = <MagicMock name='Adam' id='5223960720'>\nmock_dataloader = <MagicMock name='DataLoader' id='5223696512'>\nsample_dataset = (array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n        0.15599452, 0.05808361, 0.86617615, 0.601...34080354,\n        0.93075733, 0.85841275, 0.42899403, 0.75087107, 0.75454287]]), array([0, 1, 0, 0, 1, 1, 1, 0, 1, 0]))\n\n    @patch('torch.utils.data.DataLoader')\n    @patch('torch.optim.Adam')\n    def test_train_model(self, mock_adam, mock_dataloader, sample_dataset):\n        \"\"\"Test train_model function.\"\"\"\n        X_train, y_train = sample_dataset\n        X_val, y_val = sample_dataset  # Reuse for validation\n    \n        # Create mock miner with mock model\n        mock_miner = MagicMock()\n        mock_model = MagicMock()\n        mock_miner.model = mock_model\n    \n        # Configure the mock model to return a tensor with the right shape\n        # This is crucial to match the expected shape in BCELoss\n        sample_outputs = torch.rand(4)  # Shape matches batch size in sample_batch\n        mock_model.return_value = sample_outputs.unsqueeze(1)  # Add dimension that will be squeezed later\n    \n        # Mock DataLoader instances\n        mock_train_loader = MagicMock()\n        mock_val_loader = MagicMock()\n        mock_dataloader.side_effect = [mock_train_loader, mock_val_loader]\n    \n        # Mock optimizer\n        mock_optimizer = MagicMock()\n        mock_adam.return_value = mock_optimizer\n    \n        # Configure mock_train_loader to yield sample data\n        sample_batch = (torch.randn(4, 40), torch.randint(0, 2, (4,)).float())\n        mock_train_loader.__iter__.return_value = [sample_batch]\n        mock_train_loader.__len__.return_value = 1\n    \n        # Configure mock_val_loader to yield sample data\n        mock_val_loader.__iter__.return_value = [sample_batch]\n        mock_val_loader.__len__.return_value = 1\n    \n        # Call train_model\n>       history = train_model(\n            miner=mock_miner,\n            X_train=X_train,\n            y_train=y_train,\n            X_val=X_val,\n            y_val=y_val,\n            epochs=1,\n            batch_size=4,\n            learning_rate=0.001,\n            device=\"cpu\"\n        )\n\ntests/unit/miner/test_train.py:152: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconversion_subnet/miner/train.py:164: in train_model\n    loss.backward()\n../../../../.local/share/virtualenvs/analytics_framework-X_C0rfF4/lib/python3.10/site-packages/torch/_tensor.py:648: in backward\n    torch.autograd.backward(\n../../../../.local/share/virtualenvs/analytics_framework-X_C0rfF4/lib/python3.10/site-packages/torch/autograd/__init__.py:353: in backward\n    _engine_run_backward(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nt_outputs = (tensor(1.0249),), args = ((None,), False, False, ())\nkwargs = {'accumulate_grad': True, 'allow_unreachable': True}, attach_logging_hooks = False\n\n    def _engine_run_backward(\n        t_outputs: Sequence[Union[torch.Tensor, GradientEdge]],\n        *args: Any,\n        **kwargs: Any,\n    ) -> tuple[torch.Tensor, ...]:\n        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG\n        if attach_logging_hooks:\n            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n        try:\n>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n                t_outputs, *args, **kwargs\n            )  # Calls into the C++ engine to run the backward pass\nE           RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n\n../../../../.local/share/virtualenvs/analytics_framework-X_C0rfF4/lib/python3.10/site-packages/torch/autograd/graph.py:824: RuntimeError"
          }
        ]
      }
    }
  ],
  "metadata": {
    "generated_at": "2025-05-03T15:59:49.683061",
    "log_count": 3
  }
}