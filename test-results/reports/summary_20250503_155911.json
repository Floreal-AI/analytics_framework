{
  "exitstatus": 1,
  "test_counts": {
    "total": 52,
    "passed": 14,
    "failed": 1,
    "skipped": 0,
    "errors": 0
  },
  "duration": 0.8706810474395752,
  "failed_tests": [
    {
      "name": "tests/unit/miner/test_train.py::TestModelTraining::test_train_model",
      "duration": 0.05440312499558786,
      "error_message": "self = <test_train.TestModelTraining object at 0x125bfe3b0>\nmock_adam = <MagicMock name='Adam' id='4928933744'>\nmock_dataloader = <MagicMock name='DataLoader' id='4928936096'>\nsample_dataset = (array([[0.02058449, 0.96990985, 0.83244264, 0.21233911, 0.18182497,\n        0.18340451, 0.30424224, 0.52475643, 0.431...3200496 ,\n        0.89552323, 0.38920168, 0.01083765, 0.90538198, 0.09128668]]), array([0, 1, 1, 0, 1, 1, 0, 1, 0, 1]))\n\n    @patch('torch.utils.data.DataLoader')\n    @patch('torch.optim.Adam')\n    def test_train_model(self, mock_adam, mock_dataloader, sample_dataset):\n        \"\"\"Test train_model function.\"\"\"\n        X_train, y_train = sample_dataset\n        X_val, y_val = sample_dataset  # Reuse for validation\n    \n        # Create mock miner with mock model\n        mock_miner = MagicMock()\n        mock_model = MagicMock()\n        mock_miner.model = mock_model\n    \n        # Mock DataLoader instances\n        mock_train_loader = MagicMock()\n        mock_val_loader = MagicMock()\n        mock_dataloader.side_effect = [mock_train_loader, mock_val_loader]\n    \n        # Mock optimizer\n        mock_optimizer = MagicMock()\n        mock_adam.return_value = mock_optimizer\n    \n        # Configure mock_train_loader to yield sample data\n        sample_batch = (torch.randn(4, 40), torch.randint(0, 2, (4,)).float())\n        mock_train_loader.__iter__.return_value = [sample_batch]\n        mock_train_loader.__len__.return_value = 1\n    \n        # Configure mock_val_loader to yield sample data\n        mock_val_loader.__iter__.return_value = [sample_batch]\n        mock_val_loader.__len__.return_value = 1\n    \n        # Call train_model\n>       history = train_model(\n            miner=mock_miner,\n            X_train=X_train,\n            y_train=y_train,\n            X_val=X_val,\n            y_val=y_val,\n            epochs=1,\n            batch_size=4,\n            learning_rate=0.001,\n            device=\"cpu\"\n        )\n\ntests/unit/miner/test_train.py:147: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconversion_subnet/miner/train.py:160: in train_model\n    loss = criterion(outputs, labels)\n../../../../.local/share/virtualenvs/analytics_framework-X_C0rfF4/lib/python3.10/site-packages/torch/nn/modules/module.py:1751: in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n../../../../.local/share/virtualenvs/analytics_framework-X_C0rfF4/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: in _call_impl\n    return forward_call(*args, **kwargs)\n../../../../.local/share/virtualenvs/analytics_framework-X_C0rfF4/lib/python3.10/site-packages/torch/nn/modules/loss.py:699: in forward\n    return F.binary_cross_entropy(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ninput = <MagicMock name='mock.model().squeeze()' id='4929463456'>, target = tensor([0., 1., 1., 1.])\nweight = None, size_average = None, reduce = None, reduction = 'mean'\n\n    def binary_cross_entropy(\n        input: Tensor,\n        target: Tensor,\n        weight: Optional[Tensor] = None,\n        size_average: Optional[bool] = None,\n        reduce: Optional[bool] = None,\n        reduction: str = \"mean\",\n    ) -> Tensor:\n        r\"\"\"Measure Binary Cross Entropy between the target and input probabilities.\n    \n        See :class:`~torch.nn.BCELoss` for details.\n    \n        Args:\n            input: Tensor of arbitrary shape as probabilities.\n            target: Tensor of the same shape as input with values between 0 and 1.\n            weight (Tensor, optional): a manual rescaling weight\n                    if provided it's repeated to match input tensor shape\n            size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n                the losses are averaged over each loss element in the batch. Note that for\n                some losses, there multiple elements per sample. If the field :attr:`size_average`\n                is set to ``False``, the losses are instead summed for each minibatch. Ignored\n                when reduce is ``False``. Default: ``True``\n            reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n                losses are averaged or summed over observations for each minibatch depending\n                on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n                batch element instead and ignores :attr:`size_average`. Default: ``True``\n            reduction (str, optional): Specifies the reduction to apply to the output:\n                ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n                ``'mean'``: the sum of the output will be divided by the number of\n                elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n                and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n                specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n    \n        Examples::\n    \n            >>> input = torch.randn(3, 2, requires_grad=True)\n            >>> target = torch.rand(3, 2, requires_grad=False)\n            >>> loss = F.binary_cross_entropy(torch.sigmoid(input), target)\n            >>> loss.backward()\n        \"\"\"\n        if has_torch_function_variadic(input, target, weight):\n            return handle_torch_function(\n                binary_cross_entropy,\n                (input, target, weight),\n                input,\n                target,\n                weight=weight,\n                size_average=size_average,\n                reduce=reduce,\n                reduction=reduction,\n            )\n        if size_average is not None or reduce is not None:\n            reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n        else:\n            reduction_enum = _Reduction.get_enum(reduction)\n        if target.size() != input.size():\n>           raise ValueError(\n                f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}) is deprecated. \"\n                \"Please ensure they have the same size.\"\n            )\nE           ValueError: Using a target size (torch.Size([4])) that is different to the input size (<MagicMock name='mock.model().squeeze().size()' id='4929479744'>) is deprecated. Please ensure they have the same size.\n\n../../../../.local/share/virtualenvs/analytics_framework-X_C0rfF4/lib/python3.10/site-packages/torch/nn/functional.py:3560: ValueError"
    }
  ]
}